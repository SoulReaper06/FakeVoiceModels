{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 23:53:51.187947: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 23:53:51.295210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-31 23:53:51.295229: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-31 23:53:51.318349: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-31 23:53:51.782042: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-31 23:53:51.782099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-31 23:53:51.782105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, AddGaussianNoise\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define paths and parameters - use mel spectograms\n",
    "DATASET_PATH_TRAIN = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_train/flac\"\n",
    "DATASET_PATH_DEV = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_dev/flac\"\n",
    "DATASET_PATH_EVAL = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_eval/flac\"\n",
    "LABEL_FILE_PATH_TRAIN = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "LABEL_FILE_PATH_DEV = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "LABEL_FILE_PATH_EVAL = \"/data/common_source/datasets/asvpoof-2019-dataset/LA/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\"\n",
    "NUM_CLASSES = 2  # Number of classes (bonafide and spoof)\n",
    "SAMPLE_RATE = 16000  # Sample rate of your audio files\n",
    "DURATION = 5  # Duration of audio clips in seconds\n",
    "N_MELS = 128  # Number of Mel frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "with open(LABEL_FILE_PATH_TRAIN, 'r') as label_train:\n",
    "    lines = label_train.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    file_name = parts[1]\n",
    "    label = 0 if parts[-1] == \"bonafide\" else 1\n",
    "    labels[file_name] = label\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "max_time_steps = 109  # Define the maximum time steps for your model\n",
    "\n",
    "for file_name, label in labels.items():\n",
    "    file_path = os.path.join(DATASET_PATH_TRAIN, file_name + \".flac\")\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < max_time_steps:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "    X_train.append(mel_spectrogram)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "with open(LABEL_FILE_PATH_DEV, 'r') as label_dev:\n",
    "    lines = label_dev.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    file_name = parts[1]\n",
    "    label = 0 if parts[-1] == \"bonafide\" else 1\n",
    "    labels[file_name] = label\n",
    "\n",
    "X_dev = []\n",
    "y_dev = []\n",
    "\n",
    "max_time_steps = 109  # Define the maximum time steps for your model\n",
    "\n",
    "for file_name, label in labels.items():\n",
    "    file_path = os.path.join(DATASET_PATH_DEV, file_name + \".flac\")\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < max_time_steps:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "    X_dev.append(mel_spectrogram)\n",
    "    y_dev.append(label)\n",
    "\n",
    "X_dev = np.array(X_dev)\n",
    "y_dev = np.array(y_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "with open(LABEL_FILE_PATH_EVAL, 'r') as label_eval:\n",
    "    lines = label_eval.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split()\n",
    "    file_name = parts[1]\n",
    "    label = 0 if parts[-1] == \"bonafide\" else 1\n",
    "    labels[file_name] = label\n",
    "\n",
    "X_eval = []\n",
    "y_eval = []\n",
    "\n",
    "max_time_steps = 109  # Define the maximum time steps for your model\n",
    "\n",
    "for file_name, label in labels.items():\n",
    "    file_path = os.path.join(DATASET_PATH_EVAL, file_name + \".flac\")\n",
    "\n",
    "    # Load audio file using librosa\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "    # Extract Mel spectrogram using librosa\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Ensure all spectrograms have the same width (time steps)\n",
    "    if mel_spectrogram.shape[1] < max_time_steps:\n",
    "        mel_spectrogram = np.pad(mel_spectrogram, ((0, 0), (0, max_time_steps - mel_spectrogram.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mel_spectrogram = mel_spectrogram[:, :max_time_steps]\n",
    "\n",
    "    X_eval.append(mel_spectrogram)\n",
    "    y_eval.append(label)\n",
    "\n",
    "X_eval = np.array(X_eval)\n",
    "y_eval = np.array(y_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25380, 128, 109)\n",
      "(24844, 128, 109)\n",
      "(71237, 128, 109)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "print (X_dev.shape)\n",
    "print (X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m y_train_save_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../datasets/melSpectogram/y_train.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Save X_train and y_train as .npy files\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m np\u001b[39m.\u001b[39msave(X_train_save_path, X_train)\n\u001b[1;32m      9\u001b[0m np\u001b[39m.\u001b[39msave(y_train_save_path, y_train)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Specify the file paths where you want to save the data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Saving dava before data agumentation/normalization\n",
    "\n",
    "# Specify the file paths where you want to save the data\n",
    "X_train_save_path = '../datasets/melSpectogram/X_train.npy'\n",
    "y_train_save_path = '../datasets/melSpectogram/y_train.npy'\n",
    "\n",
    "# Save X_train and y_train as .npy files\n",
    "np.save(X_train_save_path, X_train)\n",
    "np.save(y_train_save_path, y_train)\n",
    "\n",
    "\n",
    "# Specify the file paths where you want to save the data\n",
    "X_train_save_path = '../datasets/melSpectogram/X_dev.npy'\n",
    "y_train_save_path = '../datasets/melSpectogram/y_dev.npy'\n",
    "\n",
    "# Save X_train and y_train as .npy files\n",
    "np.save(X_train_save_path, X_dev)\n",
    "np.save(y_train_save_path, y_dev)\n",
    "\n",
    "\n",
    "# Specify the file paths where you want to save the data\n",
    "X_train_save_path = '../datasets/melSpectogram/X_eval.npy'\n",
    "y_train_save_path = '../datasets/melSpectogram/y_eval.npy'\n",
    "\n",
    "# Save X_train and y_train as .npy files\n",
    "np.save(X_train_save_path, X_eval)\n",
    "np.save(y_train_save_path, y_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists to NumPy arrays\n",
    "X_train = np.load('../datasets/melSpectogram/X_train.npy')\n",
    "y_train = np.load('../datasets/melSpectogram/y_train.npy')\n",
    "y_dev = np.load('../datasets/melSpectogram/y_dev.npy')\n",
    "y_eval = np.load('../datasets/melSpectogram/y_eval.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2df73002193cefbfa9243a6efa2b74af79e3047145117f20857a75b11408ac59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

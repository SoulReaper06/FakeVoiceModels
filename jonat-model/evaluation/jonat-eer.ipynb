{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal Error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from audiomentations import Compose, TimeStretch, PitchShift, AddGaussianNoise\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import joblib\n",
    "\n",
    "X_train_augmented = np.load('/data/common_source/datasets/datasets_asvc2019/mfcc/X_train_augmented.npy')\n",
    "X_eval_augmented = np.load('/data/common_source/datasets/datasets_asvc2019/mfcc/X_eval_augmented.npy')\n",
    "y_eval = np.load('/data/common_source/datasets/datasets_asvc2019/mfcc/y_eval.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the 3D data to 2D\n",
    "X_val_2d = X_eval_augmented.reshape(-1, X_eval_augmented.shape[-1])\n",
    "X_train_2d = X_train_augmented.reshape(-1, X_train_augmented.shape[-1])\n",
    "\n",
    "# Fit the scaler to your training data and transform it\n",
    "X_train_normalized = scaler.fit_transform(X_train_2d)\n",
    "# Transform the validation and test data using the same scaler\n",
    "X_val_normalized = scaler.transform(X_val_2d)\n",
    "\n",
    "# Reshape the normalized data back to 3D\n",
    "X_val_normalized = X_val_normalized.reshape(X_eval_augmented.shape)\n",
    "\n",
    "# Shuffle the validation data (if needed)\n",
    "X_val_normalized, y_eval = shuffle(X_val_normalized, y_eval, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 04:06:51.812919: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.813052: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.813134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.813213: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.856001: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.856083: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-11-15 04:06:51.856094: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-11-15 04:06:51.856648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2227/2227 [==============================] - 18s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "model = joblib.load('jonat-model.joblib')\n",
    "\n",
    "# Make predictions on the evaluation set\n",
    "y_test_pred = model.predict(X_val_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_test is the true class labels for your test set\n",
    "target_scores = y_test_pred[y_eval == 1]  # Target scores for positive instances\n",
    "nontarget_scores = y_test_pred[y_eval == 0]  # Non-target scores for negative instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    \"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eer, eer_threshold \u001b[39m=\u001b[39m compute_eer(target_scores, nontarget_scores)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEqual Error Rate (EER): \u001b[39m\u001b[39m{\u001b[39;00meer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThreshold at EER: \u001b[39m\u001b[39m{\u001b[39;00meer_threshold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mcompute_eer\u001b[0;34m(target_scores, nontarget_scores)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_eer\u001b[39m(target_scores, nontarget_scores):\n\u001b[1;32m     23\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Returns equal error rate (EER) and the corresponding threshold. \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     frr, far, thresholds \u001b[39m=\u001b[39m compute_det_curve(target_scores, nontarget_scores)\n\u001b[1;32m     25\u001b[0m     abs_diffs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(frr \u001b[39m-\u001b[39m far)\n\u001b[1;32m     26\u001b[0m     min_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin(abs_diffs)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mcompute_det_curve\u001b[0;34m(target_scores, nontarget_scores)\u001b[0m\n\u001b[1;32m     15\u001b[0m frr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39matleast_1d(\u001b[39m0\u001b[39m), tar_trial_sums \u001b[39m/\u001b[39m target_scores\u001b[39m.\u001b[39msize))  \u001b[39m# false rejection rates\u001b[39;00m\n\u001b[1;32m     16\u001b[0m far \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39matleast_1d(\u001b[39m1\u001b[39m), nontarget_trial_sums \u001b[39m/\u001b[39m nontarget_scores\u001b[39m.\u001b[39msize))  \u001b[39m# false acceptance rates\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m thresholds \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate((np\u001b[39m.\u001b[39;49matleast_1d(all_scores[indices[\u001b[39m0\u001b[39;49m]] \u001b[39m-\u001b[39;49m \u001b[39m0.001\u001b[39;49m), all_scores[indices]))  \u001b[39m# Thresholds are the sorted scores\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m frr, far, thresholds\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 3 dimension(s)"
     ]
    }
   ],
   "source": [
    "eer, eer_threshold = compute_eer(target_scores, nontarget_scores)\n",
    "\n",
    "print(f\"Equal Error Rate (EER): {eer}\")\n",
    "print(f\"Threshold at EER: {eer_threshold}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2df73002193cefbfa9243a6efa2b74af79e3047145117f20857a75b11408ac59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-15 23:50:42.488779: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-15 23:50:43.291927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22843 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2023-10-15 23:50:43.292573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22646 MB memory:  -> device: 1, name: NVIDIA TITAN RTX, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "# code you want to evaluate\n",
    "\n",
    "\n",
    "\n",
    "# Load both Model 1 and Model 2\n",
    "model1 = load_model('models/agamjeet-model.h5') # Replace with the path to Model 1's saved file\n",
    "model2 = load_model('models/speaker_verification_model.h5') \n",
    "model3 = load_model('models/gautham-model-new.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANT VARIABLES\n",
    "MAX_PAD_LENGTH = 86\n",
    "NUM_MFCC_COEFFS = 40\n",
    "NUM_CLASSES = 2  # Number of classes (bonafide and spoof)\n",
    "SAMPLE_RATE = 16000  # Sample rate of your audio files\n",
    "DURATION = 5  # Duration of audio clips in seconds\n",
    "MAX_TIME_STEPS = 109\n",
    "N_MFCC = 13  # Number of MFCC coefficients\n",
    "HOP_LENGTH = 512  # Hop length for MFCC extraction\n",
    "WIN_LENGTH = 1024  # Window length for MFCC extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for preprocessing the data for each model\n",
    "def preprocess_data_model1(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=NUM_MFCC_COEFFS)\n",
    "        pad_width = MAX_PAD_LENGTH - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, max(0, pad_width))), mode='constant')\n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while processing file: {file_path}\")\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess a single audio file\n",
    "def preprocess_data_model2(file_path, SAMPLE_RATE, DURATION, N_MFCC, max_time_steps,HOP_LENGTH,WIN_LENGTH):\n",
    "    try:\n",
    "        # Load audio file using librosa\n",
    "        audio, _ = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
    "\n",
    "        # Extract MFCC features using librosa\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=WIN_LENGTH)\n",
    "\n",
    "        # Ensure all MFCC features have the same width (time steps)\n",
    "        if mfcc.shape[1] < max_time_steps:\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, max_time_steps - mfcc.shape[1])), mode='constant')\n",
    "        else:\n",
    "            mfcc = mfcc[:, :max_time_steps]\n",
    "            \n",
    "        return mfcc\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while processing file: {file_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_model3(file_path):\n",
    "    n_mfcc = 13\n",
    "    max_length = 100\n",
    "# Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "\n",
    "    # Ensure that the MFCCs have the same length as during training\n",
    "    if mfccs.shape[1] < max_length:\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, max_length - mfccs.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfccs = mfccs[:, :max_length]\n",
    "\n",
    "    # Reshape the MFCCs to match the input shape expected by the model\n",
    "    input_shape = (1, n_mfcc, max_length, 1)\n",
    "    mfccs = mfccs.reshape(input_shape)\n",
    "\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(predictions_model1, predictions_model2,predictions_model3):\n",
    "    # Perform majority voting\n",
    "    combined_predictions = []\n",
    "    for pred1, pred2, pred3 in zip(predictions_model1, predictions_model2, predictions_model3):\n",
    "        # Choose the class with the majority of votes\n",
    "        combined_prediction = np.argmax(np.bincount([np.argmax(pred1), np.argmax(pred2), np.argmax(pred3)]))\n",
    "        combined_predictions.append(combined_prediction)\n",
    "    \n",
    "    return np.array(combined_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0\n",
      "Prediction from Model 1: fake\n",
      "Prediction from Model 2: fake\n",
      "Prediction from Model 3: fake\n",
      "This audio sample is :  Fake\n",
      "921.0761893499875\n"
     ]
    }
   ],
   "source": [
    "audio_sample_path = '/home/jonat/MAIN PROJECT/test-audios/5sec-audio.wav'\n",
    "# Preprocess the audio sample for Model 1 (MFCC-based)\n",
    "# Ensure that the preprocessing matches what was done for Model 1\n",
    "preprocessed_audio_model1 = preprocess_data_model1(audio_sample_path)  # Implement this function\n",
    "\n",
    "# Preprocess the audio sample for Model 2 (Mel spectrogram-based)\n",
    "# Ensure that the preprocessing matches what was done for Model 2\n",
    "SAMPLE_RATE = 16000  # Sample rate of your audio files\n",
    "DURATION = 5  # Duration of audio clips in seconds\n",
    "MAX_TIME_STEPS = 109\n",
    "N_MFCC = 13  # Number of MFCC coefficients\n",
    "HOP_LENGTH = 512  # Hop length for MFCC extraction\n",
    "WIN_LENGTH = 1024  # Window length for MFCC extraction\n",
    "\n",
    "preprocessed_audio_model2 = preprocess_data_model2(audio_sample_path, SAMPLE_RATE, DURATION, N_MFCC, MAX_TIME_STEPS,HOP_LENGTH,WIN_LENGTH)\n",
    "\n",
    "preprocessed_audio_model3 = preprocess_data_model3(audio_sample_path)\n",
    "\n",
    "# Make predictions for Model 1\n",
    "prediction_model1 = model1.predict(np.expand_dims(preprocessed_audio_model2, axis=0))\n",
    "\n",
    "# Make predictions for Model 2\n",
    "prediction_model2 = model2.predict(np.expand_dims(preprocessed_audio_model2, axis=0))\n",
    "\n",
    "prediction_model3 = model3.predict(np.expand_dims(preprocessed_audio_model2, axis=0))\n",
    "\n",
    "# # Interpret the predictions\n",
    "# if prediction_model3[0][0] > 0.5:\n",
    "#     result = \"fake\"\n",
    "# else:\n",
    "#     result = \"real\"\n",
    "\n",
    "# Combine predictions using ensemble strategy (e.g., majority voting)\n",
    "ensemble_prediction = majority_vote(prediction_model1, prediction_model2,prediction_model3)  # Implement this function\n",
    "\n",
    "# Define a mapping from binary values to class labels\n",
    "class_labels = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "# Convert the ensemble_prediction array to a single integer\n",
    "ensemble_prediction_int = int(ensemble_prediction[0])\n",
    "print(ensemble_prediction_int)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "prediction_label_model1 = class_labels.get(int(prediction_model1[0][0]), \"Unknown\")\n",
    "prediction_label_model2 = class_labels.get(int(prediction_model2[0][0]), \"Unknown\")\n",
    "prediction_label_model3 = class_labels.get(int(prediction_model3[0][0]), \"Unknown\")\n",
    "\n",
    "# Print the predictions from both models\n",
    "print(f\"Prediction from Model 1: {prediction_label_model1}\")\n",
    "print(f\"Prediction from Model 2: {prediction_label_model2}\")\n",
    "print(f\"Prediction from Model 3: {prediction_label_model3}\")\n",
    "\n",
    "fakeCount = 0\n",
    "realCount = 0\n",
    "\n",
    "if(prediction_label_model1 == \"fake\"):\n",
    "    fakeCount+=1\n",
    "if(prediction_label_model2 == \"fake\"):\n",
    "    fakeCount+=1\n",
    "if(prediction_label_model3 == \"fake\"):\n",
    "    fakeCount+=1\n",
    "\n",
    "if(prediction_label_model1 == \"real\"):\n",
    "    realCount+=1\n",
    "if(prediction_label_model2 == \"real\"):\n",
    "    realCount+=1\n",
    "if(prediction_label_model3 == \"real\"):\n",
    "    realCount+=1\n",
    "\n",
    "probability_index = {}\n",
    "\n",
    "probability_index[\"Real\"] = realCount\n",
    "probability_index[\"Fake\"] = fakeCount\n",
    "\n",
    "final_prediction = max(probability_index, key= probability_index.get)\n",
    "\n",
    "print(\"This audio sample is : \", final_prediction)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
